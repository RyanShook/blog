---
title: The Gradual Drift
date: 2026-02-07 17:30:00 -0600
categories:
  - technology
  - ai
tags:
  - ai
  - software-engineering
  - systems-thinking
  - future-of-work
author: Ryan Shook
---

We're living through a narrow and unusual window in the history of computing.

Right now, most people who build software still understand the systems they're working with. We can reason about code, infrastructure, and operating systems end to end. At the same time, we have AI tools that meaningfully accelerate that work - helping us write, debug, refactor, and explore software faster than ever before.

In this moment, AI acts as leverage. It increases productivity without fully displacing understanding.

That balance is rare. And it may not last.

The central question is simple but easy to overlook:
What happens when we can no longer fix, improve upon, or really understand how our computers are working?

## A Reasonable Five-Year Projection

Projecting five years ahead doesn't require science fiction.

It's reasonable to expect that large language models will:

- Be deeply embedded in development environments
- Mediate most interaction with software systems
- Generate and modify large portions of code autonomously
- Be continuously refined through feedback and live usage

None of this implies a sudden loss of control. But it does imply a shift in where understanding lives.

Five years from now, many developers will spend less time reasoning about how systems work internally and more time evaluating outputs, constraints, and observed behavior. Software development becomes less about writing logic and more about supervising it.

That shift is subtle, but structural.

## From Authors to Evaluators

Historically, software development has been grounded in explicit logic. Humans define the rules; machines execute them. With AI-mediated systems, humans increasingly define goals and constraints, while systems discover the logic that satisfies them.

Agency doesn't disappear, but it changes shape.

The core skill moves from construction to oversight, from authorship to evaluation. Instead of asking "How does this work?", the dominant question becomes "Is this behaving acceptably?"

Over time, that difference compounds.

When understanding becomes optional, fewer people maintain it. When fewer people maintain it, repair and improvement become harder - not impossible, but less reliable.

## The Coordination Layer Becomes the System

LLMs are unlikely to replace operating systems in a technical sense. Kernels, schedulers, and memory managers will still exist.

But for most users - and increasingly for developers - the AI layer becomes the system they actually interact with.

It shapes:

- How code is structured
- Which patterns are reused
- How errors are resolved
- How systems evolve over time

Understanding the underlying layers remains possible, but less necessary - and therefore less common. Human comprehension slowly shifts from being central to being peripheral.

This is not a failure state. It's a drift.

## Where the Limits Become Visible

The tension becomes clearest at the boundary between software and the physical world.

Software can iterate rapidly. It can retrain, optimize, and refine itself continuously. Hardware cannot. It remains constrained by physics, materials, energy, and time.

Five years from now, it's plausible that:

- Software systems increasingly define their own performance targets
- Hardware upgrades are driven by AI-determined requirements
- Humans carry out the physical changes without shaping the system's underlying logic

This isn't new in principle - physical labor has always supported abstract systems - but the novelty lies in agency. Humans may increasingly maintain infrastructure for systems they did not design and cannot meaningfully modify.

## Not a Crisis, but a Drift

This isn't an argument about AI rebellion, takeover, or catastrophe.

There is no sudden breaking point.

Instead, there is gradual erosion. Humans remain in the loop, but increasingly at the level of approval rather than creation. We retain operational control, but lose conceptual leverage. Systems continue to function, but fewer people can explain why they function the way they do.

The system works - until it doesn't. And when it doesn't, repair becomes slower, more expensive, and more fragile.

## Why the Question Matters Now

The importance of this moment isn't that AI exists. It's that we still retain fluency in the systems AI is beginning to mediate.

Five years from now, that fluency may be rarer - not because it's impossible, but because it's no longer required to be productive.

So the question isn't whether AI will advance. It will.

The question is whether we are comfortable with a world in which the core operating logic of society - software - continues to evolve beyond our ability to fix it, improve it, or truly understand it.

That decision isn't made all at once.
It's made gradually.
